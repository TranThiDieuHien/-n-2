{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPcsHt+i6CYkaC9ktOG/pEK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TranThiDieuHien/Do_An_2/blob/main/Pose_Detection_tflite_opencv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tflite-runtime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fs_1XBAby0rn",
        "outputId": "316e75be-2449-459d-8fc7-cf51397dc597"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tflite-runtime\n",
            "  Downloading tflite_runtime-2.14.0-cp310-cp310-manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.10/dist-packages (from tflite-runtime) (1.25.2)\n",
            "Installing collected packages: tflite-runtime\n",
            "Successfully installed tflite-runtime-2.14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "9oyS565HyG0O"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Run Pose detection on images, Press ESC to exit the program\n",
        "For Raspberry PI, please use `import tflite_runtime.interpreter as tflite` instead\n",
        "\"\"\"\n",
        "import re\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow.lite as tflite\n",
        "import tflite_runtime.interpreter as tflite\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "class Part:\n",
        "    r\"\"\"Enum of Detected Part IDs, for example, 0 is Nose\"\"\"\n",
        "    NOSE = 0\n",
        "    LEFT_EYE = 1\n",
        "    RIGHT_EYE = 2\n",
        "    LEFT_EAR = 3\n",
        "    RIGHT_EAR = 4,\n",
        "    LEFT_SHOULDER = 5\n",
        "    RIGHT_SHOULDER = 6\n",
        "    LEFT_ELBOW = 7\n",
        "    RIGHT_ELBOW = 8\n",
        "    LEFT_WRIST = 9\n",
        "    RIGHT_WRIST = 10\n",
        "    LEFT_HIP = 11\n",
        "    RIGHT_HIP = 12\n",
        "    LEFT_KNEE = 13\n",
        "    RIGHT_KNEE = 14\n",
        "    LEFT_ANKLE = 15\n",
        "    RIGHT_ANKLE = 16"
      ],
      "metadata": {
        "id": "ZqSFLXzeyUoY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "DqNLeHSY0yDY"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "    return 1.0 / (1.0 + 1.0 / np.exp(x))\n",
        "\n",
        "\n",
        "def load_model(model_path):\n",
        "    r\"\"\"Load TFLite model, returns a Interpreter instance.\"\"\"\n",
        "    interpreter = tflite.Interpreter(model_path=model_path)\n",
        "    interpreter.allocate_tensors()\n",
        "    return interpreter\n",
        "\n",
        "\n",
        "def process_image(interpreter, image, input_index):\n",
        "    r\"\"\"Process an image, Return a list of positions in a 4-Tuple (pos_x, pos_y, offset_x, offset_y)\"\"\"\n",
        "    input_data = np.expand_dims(image, axis=0)  # expand to 4-dim\n",
        "    input_data = (np.float32(input_data) - 127.5) / 127.5  # float point\n",
        "\n",
        "    # Process\n",
        "    interpreter.set_tensor(input_index, input_data)\n",
        "    interpreter.invoke()\n",
        "\n",
        "    # Get outputs\n",
        "    output_details = interpreter.get_output_details()\n",
        "    # print(output_details)\n",
        "\n",
        "    output_data = np.squeeze(\n",
        "        interpreter.get_tensor(output_details[0]['index']))\n",
        "    offset_data = np.squeeze(\n",
        "        interpreter.get_tensor(output_details[1]['index']))\n",
        "\n",
        "    points = []\n",
        "\n",
        "    total_row, total_col, total_points = output_data.shape\n",
        "\n",
        "    # totally 17 points\n",
        "    for k in range(total_points):\n",
        "        max_score = output_data[0][0][k]\n",
        "        max_row = 0\n",
        "        max_col = 0\n",
        "        for row in range(total_row):\n",
        "            for col in range(total_col):\n",
        "                if (output_data[row][col][k] > max_score):\n",
        "                    max_score = output_data[row][col][k]\n",
        "                    max_row = row\n",
        "                    max_col = col\n",
        "\n",
        "        points.append((max_row, max_col))\n",
        "        # print(sigmoid(max_score))\n",
        "\n",
        "    positions = []\n",
        "\n",
        "    for idx, point in enumerate(points):\n",
        "        pos_y, pos_x = point\n",
        "\n",
        "        # y is row, x is column\n",
        "        offset_x = offset_data[pos_y][pos_x][idx + 17]\n",
        "        offset_y = offset_data[pos_y][pos_x][idx]\n",
        "\n",
        "        positions.append((pos_x, pos_y, offset_x, offset_y))\n",
        "        # confidenceScores = sigmoid(output_data[pos_y][pos_x][idx])\n",
        "        # print('confidenceScores {}'.format(confidenceScores))\n",
        "\n",
        "    return positions\n",
        "\n",
        "\n",
        "def display_result(positions, frame):\n",
        "    r\"\"\"Display Detected Points in circles\"\"\"\n",
        "    size = 5\n",
        "    color = (255, 0, 0)  # Blue color\n",
        "    thickness = 3\n",
        "\n",
        "    width = frame.shape[1]\n",
        "    height = frame.shape[0]\n",
        "\n",
        "    for pos in positions:\n",
        "        pos_x, pos_y, offset_x, offset_y = pos\n",
        "\n",
        "        # Calculating the x and y coordinates\n",
        "        x = int(pos_x / 8 * width + offset_x)\n",
        "        y = int(pos_y / 8 * height + offset_y)\n",
        "\n",
        "        cv2.circle(frame, (x, y), size, color, thickness)\n",
        "\n",
        "    cv2.imshow('Pose Detection', frame)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    model_path = 'posenet_mobilenet_v1_100_257x257_multi_kpt_stripped.tflite'\n",
        "    image_path = 'person.jpg'\n",
        "\n",
        "    interpreter = load_model(model_path)\n",
        "\n",
        "    input_details = interpreter.get_input_details()\n",
        "    # Get Width and Height\n",
        "    input_shape = input_details[0]['shape']\n",
        "    height = input_shape[1]\n",
        "    width = input_shape[2]\n",
        "\n",
        "    # Get input index\n",
        "    input_index = input_details[0]['index']\n",
        "\n",
        "    frame = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
        "    print(frame.shape)\n",
        "\n",
        "    image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "    image = image.resize((width, height))\n",
        "\n",
        "    positions = process_image(interpreter, image, input_index)\n",
        "    display_result(positions, frame)\n",
        "\n",
        "    key = cv2.waitKey(0)\n",
        "    if key == 27:  # esc\n",
        "        cv2.destroyAllWindows()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "i4vaWvjZyc7f",
        "outputId": "e99f0de4-f31e-4c2c-9c77-a6b1b3456f80"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1000, 700, 3)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "DisabledFunctionError",
          "evalue": "cv2.imshow() is disabled in Colab, because it causes Jupyter sessions\nto crash; see https://github.com/jupyter/notebook/issues/3935.\nAs a substitution, consider using\n  from google.colab.patches import cv2_imshow\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mDisabledFunctionError\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-e4af2359f740>\u001b[0m in \u001b[0;36m<cell line: 86>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0mpositions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minterpreter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0mdisplay_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-e4af2359f740>\u001b[0m in \u001b[0;36mdisplay_result\u001b[0;34m(positions, frame)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcircle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthickness\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Pose Detection'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_import_hooks/_cv2.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mDisabledFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mDisabledFunctionError\u001b[0m: cv2.imshow() is disabled in Colab, because it causes Jupyter sessions\nto crash; see https://github.com/jupyter/notebook/issues/3935.\nAs a substitution, consider using\n  from google.colab.patches import cv2_imshow\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_snippet",
                "actionText": "Search Snippets for cv2.imshow",
                "snippetFilter": "cv2.imshow"
              }
            ]
          }
        }
      ]
    }
  ]
}